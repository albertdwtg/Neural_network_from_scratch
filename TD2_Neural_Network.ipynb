{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD2_Neural_Network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Exercice 1"
      ],
      "metadata": {
        "id": "kjeFlh2EEuG1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MYDlY-Q_Caqj"
      },
      "outputs": [],
      "source": [
        "from random import seed \n",
        "from random import random\n",
        "\n",
        "# Initialize a network\n",
        "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
        "\tnetwork = list()\n",
        "\thidden_layer = [{'weights':[round(random(),1) for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
        "\tnetwork.append(hidden_layer)\n",
        "\toutput_layer = [{'weights':[round(random(),1) for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
        "\tnetwork.append(output_layer)\n",
        "\treturn network\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a network\n",
        "def initialize_network2(n_inputs, n_hidden, n_outputs):\n",
        "  network = list()\n",
        "  previous=n_inputs + 1\n",
        "  for a in range(len(n_hidden)):\n",
        "    hidden_layer = [{'weights':[round(random(),1) for i in range(previous)]} for i in range(n_hidden[a])]\n",
        "    previous=n_hidden[a]+1\n",
        "    network.append(hidden_layer)\n",
        "  output_layer = [{'weights':[round(random(),1) for i in range(previous)]} for i in range(n_outputs)]\n",
        "  network.append(output_layer)\n",
        "  return network"
      ],
      "metadata": {
        "id": "uJDnBAu6RTHP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write(network):\n",
        "  for i in range(len(network)):\n",
        "    layer = network[i]\n",
        "    print('layer :',i+1,'\\n')\n",
        "    for neuron in layer:\n",
        "\t    print(neuron,'\\n')\n"
      ],
      "metadata": {
        "id": "1jaS-PTRE3y-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network=initialize_network(2,5,4)\n",
        "write(network)\n",
        "#last value of weights is the constant"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COitN4HEUJfu",
        "outputId": "56938241-2939-4111-bf49-c209bc8c9815"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layer : 1 \n",
            "\n",
            "{'weights': [0.9, 0.7, 1.0]} \n",
            "\n",
            "{'weights': [0.1, 0.1, 0.6]} \n",
            "\n",
            "{'weights': [0.4, 0.4, 0.6]} \n",
            "\n",
            "{'weights': [0.9, 0.9, 0.3]} \n",
            "\n",
            "{'weights': [0.7, 0.3, 0.3]} \n",
            "\n",
            "layer : 2 \n",
            "\n",
            "{'weights': [0.2, 0.4, 0.5, 0.6, 0.2, 1.0]} \n",
            "\n",
            "{'weights': [0.8, 0.8, 0.6, 1.0, 0.1, 0.4]} \n",
            "\n",
            "{'weights': [0.5, 0.3, 0.2, 0.5, 0.4, 0.5]} \n",
            "\n",
            "{'weights': [0.6, 0.7, 1.0, 0.3, 0.4, 0.8]} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "network2=initialize_network2(2,[4,2,2],4)\n",
        "write(network2)\n",
        "#last value of weights is the constant"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSFqeeh-E5RR",
        "outputId": "37ae0870-b4f3-40e2-a5a5-3acd76ab5d46"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layer : 1 \n",
            "\n",
            "{'weights': [0.8, 0.9, 1.0]} \n",
            "\n",
            "{'weights': [0.5, 0.0, 0.1]} \n",
            "\n",
            "{'weights': [0.5, 0.2, 0.1]} \n",
            "\n",
            "{'weights': [0.6, 0.5, 0.8]} \n",
            "\n",
            "layer : 2 \n",
            "\n",
            "{'weights': [0.6, 0.7, 0.3, 0.9, 0.6]} \n",
            "\n",
            "{'weights': [0.7, 0.2, 0.3, 0.4, 0.2]} \n",
            "\n",
            "layer : 3 \n",
            "\n",
            "{'weights': [0.2, 0.2, 0.4]} \n",
            "\n",
            "{'weights': [0.1, 0.9, 0.8]} \n",
            "\n",
            "layer : 4 \n",
            "\n",
            "{'weights': [0.5, 0.3, 0.3]} \n",
            "\n",
            "{'weights': [0.2, 0.3, 0.9]} \n",
            "\n",
            "{'weights': [0.1, 0.7, 0.9]} \n",
            "\n",
            "{'weights': [0.2, 0.9, 0.1]} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate neuron activation for an input #this is the in function\n",
        "def activate(weights, inputs):\n",
        "\tactivation = weights[-1]\n",
        "\tfor i in range(len(weights)-1):\n",
        "\t\tactivation += weights[i] * inputs[i]\n",
        "\treturn activation\n"
      ],
      "metadata": {
        "id": "ZlZeWUGQIDSg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights=[0.2,0.5,0.2,1]\n",
        "inputs=[7,5,9]\n",
        "activate(weights,inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJA6yhPXIFTN",
        "outputId": "08692b7a-d334-4cbe-b612-53f41d04dca9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.7"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from math import exp\n",
        "# Transfer neuron activation\n",
        "def transfer(activation):\n",
        "\treturn 1.0 / (1.0 + exp(-activation))\n",
        "\n",
        "transfer(0.79*0.9+0.5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3O0FqkaKkqJ",
        "outputId": "c49e2e36-ba0f-400d-ec70-9d60e265fcce"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7704758396966486"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercice 5"
      ],
      "metadata": {
        "id": "sg3AkS67Kvvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward propagate input to a network output\n",
        "def forward_propagate(network, row):\n",
        "\tinputs = row\n",
        "\tfor layer in network:\n",
        "\t\tnew_inputs = []\n",
        "\t\tfor neuron in layer:\n",
        "\t\t\tactivation = activate(neuron['weights'], inputs)\n",
        "\t\t\tneuron['output'] = transfer(activation)\n",
        "\t\t\tnew_inputs.append(neuron['output'])\n",
        "\t\tinputs = new_inputs\n",
        "\treturn inputs\n"
      ],
      "metadata": {
        "id": "p33Ycxm6KmSF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercice 6"
      ],
      "metadata": {
        "id": "LzavXnvoNF3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test forward propagation\n",
        "network = initialize_network(2, 1,2)\n",
        "print('######Before#########')\n",
        "write(network)\n",
        "row = [1, 0]\n",
        "output = forward_propagate(network, row)\n",
        "print('######After#########')\n",
        "write(network)\n",
        "\n",
        "print('#####output##########')\n",
        "print('output:',output,'\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkEw8Qf-NIL1",
        "outputId": "49a82b93-91d6-44e5-fe50-200e64173d77"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "######Before#########\n",
            "layer : 1 \n",
            "\n",
            "{'weights': [0.9, 0.1, 0.4]} \n",
            "\n",
            "layer : 2 \n",
            "\n",
            "{'weights': [0.9, 0.5]} \n",
            "\n",
            "{'weights': [0.2, 0.6]} \n",
            "\n",
            "######After#########\n",
            "layer : 1 \n",
            "\n",
            "{'weights': [0.9, 0.1, 0.4], 'output': 0.7858349830425586} \n",
            "\n",
            "layer : 2 \n",
            "\n",
            "{'weights': [0.9, 0.5], 'output': 0.7698122696812021} \n",
            "\n",
            "{'weights': [0.2, 0.6], 'output': 0.6807383423288406} \n",
            "\n",
            "#####output##########\n",
            "output: [0.7698122696812021, 0.6807383423288406] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercice 7"
      ],
      "metadata": {
        "id": "cDuLaavfO_wH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the derivative of an neuron output\n",
        "def transfer_derivative(output):\n",
        "\treturn output * (1.0 - output)\n"
      ],
      "metadata": {
        "id": "inPFCWERO9Zp"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercice 8"
      ],
      "metadata": {
        "id": "1bxoIK7iQFau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Backpropagate error and store in neurons\n",
        "def backward_propagate_error(network, expected):\n",
        "\tfor i in reversed(range(len(network))):\n",
        "\t\tlayer = network[i]\n",
        "\t\terrors = list()\n",
        "\t\tif i != len(network)-1:\n",
        "\t\t\tfor j in range(len(layer)):\n",
        "\t\t\t\terror = 0.0\n",
        "\t\t\t\tfor neuron in network[i + 1]:\n",
        "\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n",
        "\t\t\t\terrors.append(error)\n",
        "\t\telse:\n",
        "\t\t\tfor j in range(len(layer)):\n",
        "\t\t\t\tneuron = layer[j]\n",
        "\t\t\t\terrors.append(expected[j] - neuron['output'])\n",
        "\t\tfor j in range(len(layer)):\n",
        "\t\t\tneuron = layer[j]\n",
        "\t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n"
      ],
      "metadata": {
        "id": "tPosaRGzQC8a"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercice 9"
      ],
      "metadata": {
        "id": "uI2XS_7TUTGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test backpropagation of error\n",
        "expected = [0, 1]\n",
        "print('######Before#########')\n",
        "write(network)\n",
        "backward_propagate_error(network, expected)\n",
        "print('######After#########')\n",
        "write(network)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1iU74yZQfrB",
        "outputId": "704cfec5-ca04-4c24-bb10-fe0c41341f39"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "######Before#########\n",
            "layer : 1 \n",
            "\n",
            "{'weights': [0.9, 0.1, 0.4], 'output': 0.7858349830425586} \n",
            "\n",
            "layer : 2 \n",
            "\n",
            "{'weights': [0.9, 0.5], 'output': 0.7698122696812021} \n",
            "\n",
            "{'weights': [0.2, 0.6], 'output': 0.6807383423288406} \n",
            "\n",
            "######After#########\n",
            "layer : 1 \n",
            "\n",
            "{'weights': [0.9, 0.1, 0.4], 'output': 0.7858349830425586, 'delta': -0.0183265688169951} \n",
            "\n",
            "layer : 2 \n",
            "\n",
            "{'weights': [0.9, 0.5], 'output': 0.7698122696812021, 'delta': -0.13641176506581207} \n",
            "\n",
            "{'weights': [0.2, 0.6], 'output': 0.6807383423288406, 'delta': 0.06938630188144451} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercice 10"
      ],
      "metadata": {
        "id": "saDe7ZAmXTzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update network weights with error\n",
        "def update_weights(network, row, l_rate):\n",
        "\tfor i in range(len(network)):\n",
        "\t\tinputs = row[:-1]\n",
        "\t\tif i != 0:\n",
        "\t\t\tinputs = [neuron['output'] for neuron in network[i - 1]]\n",
        "\t\tfor neuron in network[i]:\n",
        "\t\t\tfor j in range(len(inputs)):\n",
        "\t\t\t\tneuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
        "\t\t\tneuron['weights'][-1] += l_rate * neuron['delta']\n"
      ],
      "metadata": {
        "id": "Q5R_YDTZKxQ9"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # Train a network for a fixed number of epochs\n",
        "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
        "\tfor epoch in range(n_epoch):\n",
        "\t\tsum_error = 0\n",
        "\t\tfor row in train:\n",
        "\t\t\toutputs = forward_propagate(network, row)\n",
        "\t\t\texpected = [0 for i in range(n_outputs)]\n",
        "\t\t\texpected[row[-1]] = 1\n",
        "\t\t\tsum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
        "\t\t\tbackward_propagate_error(network, expected)\n",
        "\t\t\tupdate_weights(network, row, l_rate)\n",
        "\t\tprint('>epoch=%d, lrate=%.3f, error=%.5f' % (epoch, l_rate, sum_error))\n"
      ],
      "metadata": {
        "id": "_liVRENBK8K9"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test training backprop algorithm\n",
        "#seed(1)\n",
        "dataset = [[2.7810836,2.550537003,0],\n",
        "\t[1.465489372,2.362125076,0],\n",
        "\t[3.396561688,4.400293529,0],\n",
        "\t[1.38807019,1.850220317,0],\n",
        "\t[3.06407232,3.005305973,0],\n",
        "\t[7.627531214,2.759262235,1],\n",
        "\t[5.332441248,2.088626775,1],\n",
        "\t[6.922596716,1.77106367,1],\n",
        "\t[8.675418651,-0.242068655,1],\n",
        "\t[7.673756466,3.508563011,1],\n",
        "  [2.156498,9.185498484,0],\n",
        "  [6.1548451326548,4.2165498465165,1],\n",
        "  [5.021265465,1.1564011,0],\n",
        "  [-0.65548,3.45668468468,1],\n",
        "  [2.254848,7.1165465,0],\n",
        "  [9.2165184185,4.0659845984,1],\n",
        "  [1.0215165,3.5498,0]]\n",
        "n_inputs=len(dataset[0])-1\n",
        "n_outputs=len(set([row[-1] for row in dataset]))\n",
        "\n",
        "network=initialize_network(n_inputs,2,n_outputs)\n",
        "train_network(network,dataset,0.8,50,n_outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjpLwPBzMmzz",
        "outputId": "9863b44c-c8ce-4980-85a6-fde1a0370a54"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">epoch=0, lrate=0.800, error=10.12734\n",
            ">epoch=1, lrate=0.800, error=9.33165\n",
            ">epoch=2, lrate=0.800, error=9.31477\n",
            ">epoch=3, lrate=0.800, error=9.31194\n",
            ">epoch=4, lrate=0.800, error=9.30790\n",
            ">epoch=5, lrate=0.800, error=9.30310\n",
            ">epoch=6, lrate=0.800, error=9.29759\n",
            ">epoch=7, lrate=0.800, error=9.29134\n",
            ">epoch=8, lrate=0.800, error=9.28431\n",
            ">epoch=9, lrate=0.800, error=9.27633\n",
            ">epoch=10, lrate=0.800, error=9.26695\n",
            ">epoch=11, lrate=0.800, error=9.25509\n",
            ">epoch=12, lrate=0.800, error=9.23845\n",
            ">epoch=13, lrate=0.800, error=9.21153\n",
            ">epoch=14, lrate=0.800, error=9.15641\n",
            ">epoch=15, lrate=0.800, error=8.98836\n",
            ">epoch=16, lrate=0.800, error=8.41844\n",
            ">epoch=17, lrate=0.800, error=7.52826\n",
            ">epoch=18, lrate=0.800, error=6.83323\n",
            ">epoch=19, lrate=0.800, error=6.31634\n",
            ">epoch=20, lrate=0.800, error=5.93070\n",
            ">epoch=21, lrate=0.800, error=5.65781\n",
            ">epoch=22, lrate=0.800, error=5.47814\n",
            ">epoch=23, lrate=0.800, error=5.35884\n",
            ">epoch=24, lrate=0.800, error=5.26686\n",
            ">epoch=25, lrate=0.800, error=5.17843\n",
            ">epoch=26, lrate=0.800, error=5.08661\n",
            ">epoch=27, lrate=0.800, error=4.99566\n",
            ">epoch=28, lrate=0.800, error=4.90918\n",
            ">epoch=29, lrate=0.800, error=4.82800\n",
            ">epoch=30, lrate=0.800, error=4.75162\n",
            ">epoch=31, lrate=0.800, error=4.67882\n",
            ">epoch=32, lrate=0.800, error=4.60824\n",
            ">epoch=33, lrate=0.800, error=4.53838\n",
            ">epoch=34, lrate=0.800, error=4.46786\n",
            ">epoch=35, lrate=0.800, error=4.39550\n",
            ">epoch=36, lrate=0.800, error=4.32073\n",
            ">epoch=37, lrate=0.800, error=4.24429\n",
            ">epoch=38, lrate=0.800, error=4.16907\n",
            ">epoch=39, lrate=0.800, error=4.09988\n",
            ">epoch=40, lrate=0.800, error=4.04114\n",
            ">epoch=41, lrate=0.800, error=3.99400\n",
            ">epoch=42, lrate=0.800, error=3.95650\n",
            ">epoch=43, lrate=0.800, error=3.92605\n",
            ">epoch=44, lrate=0.800, error=3.90070\n",
            ">epoch=45, lrate=0.800, error=3.87922\n",
            ">epoch=46, lrate=0.800, error=3.86076\n",
            ">epoch=47, lrate=0.800, error=3.84472\n",
            ">epoch=48, lrate=0.800, error=3.83065\n",
            ">epoch=49, lrate=0.800, error=3.81823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a prediction with a network\n",
        "def predict(network, row):\n",
        "\toutputs = forward_propagate(network, row)\n",
        "\treturn outputs.index(max(outputs))\n"
      ],
      "metadata": {
        "id": "E8ybMiHhUEd8"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exo 14\n"
      ],
      "metadata": {
        "id": "H2pEgfFzULms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test making predictions with the network\n",
        "dataset = [[2.7810836,2.550537003,0],\n",
        "\t[1.465489372,2.362125076,0],\n",
        "\t[3.396561688,4.400293529,0],\n",
        "\t[1.38807019,1.850220317,0],\n",
        "\t[3.06407232,3.005305973,0],\n",
        "\t[7.627531214,2.759262235,1],\n",
        "\t[5.332441248,2.088626775,1],\n",
        "\t[6.922596716,1.77106367,1],\n",
        "\t[8.675418651,-0.242068655,1],\n",
        "\t[7.673756466,3.508563011,1]]\n",
        "#network = [[{'weights': [-1.482313569067226, 1.8308790073202204, 1.078381922048799]}, {'weights': [0.23244990332399884, 0.3621998343835864, 0.40289821191094327]}],\n",
        "#\t[{'weights': [2.5001872433501404, 0.7887233511355132, -1.1026649757805829]}, {'weights': [-2.429350576245497, 0.8357651039198697, 1.0699217181280656]}]]\n",
        "for row in dataset:\n",
        "\tprediction = predict(network, row)\n",
        "\tprint('Expected=%d, Got=%d' % (row[-1], prediction))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQy9aPzjUKWR",
        "outputId": "ccf330bd-df60-4722-c0a8-404ec01508a9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected=0, Got=0\n",
            "Expected=0, Got=0\n",
            "Expected=0, Got=0\n",
            "Expected=0, Got=0\n",
            "Expected=0, Got=0\n",
            "Expected=1, Got=1\n",
            "Expected=1, Got=1\n",
            "Expected=1, Got=1\n",
            "Expected=1, Got=1\n",
            "Expected=1, Got=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercice 15\n"
      ],
      "metadata": {
        "id": "LyI8xnyUVaS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "xrtVx5YWVYcI",
        "outputId": "4c00ac59-1b87-4d36-cbbc-33ea252f3345"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0f48be1d-c147-4225-8713-82f43bfefa9b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0f48be1d-c147-4225-8713-82f43bfefa9b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving seeds_dataset.csv to seeds_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Backprop on the Seeds Dataset\n",
        "from random import seed\n",
        "from random import randrange\n",
        "from random import random\n",
        "from csv import reader\n",
        "from math import exp\n"
      ],
      "metadata": {
        "id": "qdXsaYxxVZtl"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import seed\n",
        "from random import randrange\n",
        "from random import random\n",
        "from csv import reader\n",
        "from math import exp\n",
        "import pandas as pd\n",
        "\n",
        "def load_csv(filename):\n",
        "\tdataset = list()\n",
        "\twith open(filename, 'r') as file:\n",
        "\t\tcsv_reader = reader(file, delimiter=',')\n",
        "\t\tfor row in csv_reader:\n",
        "\t\t\tif not row:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\tdataset.append(row)\n",
        "\treturn dataset\n",
        "\n",
        "df = load_csv('seeds_dataset.csv')\n",
        "dff = list()\n",
        "for row in df:\n",
        "    l = row[0].split('\\t')\n",
        "    row = [float(i) for i in l] \n",
        "    row[-1] = int(row[-1]-1)   \n",
        "    dff.append(row)\n",
        "dff\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxA_xJ58Vkd7",
        "outputId": "96e1649a-f312-4576-d877-e78bfe7c772c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[15.26, 14.84, 0.871, 5.763, 3.312, 2.221, 5.22, 0],\n",
              " [14.88, 14.57, 0.8811, 5.554, 3.333, 1.018, 4.956, 0],\n",
              " [14.29, 14.09, 0.905, 5.291, 3.337, 2.699, 4.825, 0],\n",
              " [13.84, 13.94, 0.8955, 5.324, 3.379, 2.259, 4.805, 0],\n",
              " [16.14, 14.99, 0.9034, 5.658, 3.562, 1.355, 5.175, 0],\n",
              " [14.38, 14.21, 0.8951, 5.386, 3.312, 2.462, 4.956, 0],\n",
              " [14.69, 14.49, 0.8799, 5.563, 3.259, 3.586, 5.219, 0],\n",
              " [14.11, 14.1, 0.8911, 5.42, 3.302, 2.7, 5.0, 0],\n",
              " [16.63, 15.46, 0.8747, 6.053, 3.465, 2.04, 5.877, 0],\n",
              " [16.44, 15.25, 0.888, 5.884, 3.505, 1.969, 5.533, 0],\n",
              " [15.26, 14.85, 0.8696, 5.714, 3.242, 4.543, 5.314, 0],\n",
              " [14.03, 14.16, 0.8796, 5.438, 3.201, 1.717, 5.001, 0],\n",
              " [13.89, 14.02, 0.888, 5.439, 3.199, 3.986, 4.738, 0],\n",
              " [13.78, 14.06, 0.8759, 5.479, 3.156, 3.136, 4.872, 0],\n",
              " [13.74, 14.05, 0.8744, 5.482, 3.114, 2.932, 4.825, 0],\n",
              " [14.59, 14.28, 0.8993, 5.351, 3.333, 4.185, 4.781, 0],\n",
              " [13.99, 13.83, 0.9183, 5.119, 3.383, 5.234, 4.781, 0],\n",
              " [15.69, 14.75, 0.9058, 5.527, 3.514, 1.599, 5.046, 0],\n",
              " [14.7, 14.21, 0.9153, 5.205, 3.466, 1.767, 4.649, 0],\n",
              " [12.72, 13.57, 0.8686, 5.226, 3.049, 4.102, 4.914, 0],\n",
              " [14.16, 14.4, 0.8584, 5.658, 3.129, 3.072, 5.176, 0],\n",
              " [14.11, 14.26, 0.8722, 5.52, 3.168, 2.688, 5.219, 0],\n",
              " [15.88, 14.9, 0.8988, 5.618, 3.507, 0.7651, 5.091, 0],\n",
              " [12.08, 13.23, 0.8664, 5.099, 2.936, 1.415, 4.961, 0],\n",
              " [15.01, 14.76, 0.8657, 5.789, 3.245, 1.791, 5.001, 0],\n",
              " [16.19, 15.16, 0.8849, 5.833, 3.421, 0.903, 5.307, 0],\n",
              " [13.02, 13.76, 0.8641, 5.395, 3.026, 3.373, 4.825, 0],\n",
              " [12.74, 13.67, 0.8564, 5.395, 2.956, 2.504, 4.869, 0],\n",
              " [14.11, 14.18, 0.882, 5.541, 3.221, 2.754, 5.038, 0],\n",
              " [13.45, 14.02, 0.8604, 5.516, 3.065, 3.531, 5.097, 0],\n",
              " [13.16, 13.82, 0.8662, 5.454, 2.975, 0.8551, 5.056, 0],\n",
              " [15.49, 14.94, 0.8724, 5.757, 3.371, 3.412, 5.228, 0],\n",
              " [14.09, 14.41, 0.8529, 5.717, 3.186, 3.92, 5.299, 0],\n",
              " [13.94, 14.17, 0.8728, 5.585, 3.15, 2.124, 5.012, 0],\n",
              " [15.05, 14.68, 0.8779, 5.712, 3.328, 2.129, 5.36, 0],\n",
              " [16.12, 15.0, 0.9, 5.709, 3.485, 2.27, 5.443, 0],\n",
              " [16.2, 15.27, 0.8734, 5.826, 3.464, 2.823, 5.527, 0],\n",
              " [17.08, 15.38, 0.9079, 5.832, 3.683, 2.956, 5.484, 0],\n",
              " [14.8, 14.52, 0.8823, 5.656, 3.288, 3.112, 5.309, 0],\n",
              " [14.28, 14.17, 0.8944, 5.397, 3.298, 6.685, 5.001, 0],\n",
              " [13.54, 13.85, 0.8871, 5.348, 3.156, 2.587, 5.178, 0],\n",
              " [13.5, 13.85, 0.8852, 5.351, 3.158, 2.249, 5.176, 0],\n",
              " [13.16, 13.55, 0.9009, 5.138, 3.201, 2.461, 4.783, 0],\n",
              " [15.5, 14.86, 0.882, 5.877, 3.396, 4.711, 5.528, 0],\n",
              " [15.11, 14.54, 0.8986, 5.579, 3.462, 3.128, 5.18, 0],\n",
              " [13.8, 14.04, 0.8794, 5.376, 3.155, 1.56, 4.961, 0],\n",
              " [15.36, 14.76, 0.8861, 5.701, 3.393, 1.367, 5.132, 0],\n",
              " [14.99, 14.56, 0.8883, 5.57, 3.377, 2.958, 5.175, 0],\n",
              " [14.79, 14.52, 0.8819, 5.545, 3.291, 2.704, 5.111, 0],\n",
              " [14.86, 14.67, 0.8676, 5.678, 3.258, 2.129, 5.351, 0],\n",
              " [14.43, 14.4, 0.8751, 5.585, 3.272, 3.975, 5.144, 0],\n",
              " [15.78, 14.91, 0.8923, 5.674, 3.434, 5.593, 5.136, 0],\n",
              " [14.49, 14.61, 0.8538, 5.715, 3.113, 4.116, 5.396, 0],\n",
              " [14.33, 14.28, 0.8831, 5.504, 3.199, 3.328, 5.224, 0],\n",
              " [14.52, 14.6, 0.8557, 5.741, 3.113, 1.481, 5.487, 0],\n",
              " [15.03, 14.77, 0.8658, 5.702, 3.212, 1.933, 5.439, 0],\n",
              " [14.46, 14.35, 0.8818, 5.388, 3.377, 2.802, 5.044, 0],\n",
              " [14.92, 14.43, 0.9006, 5.384, 3.412, 1.142, 5.088, 0],\n",
              " [15.38, 14.77, 0.8857, 5.662, 3.419, 1.999, 5.222, 0],\n",
              " [12.11, 13.47, 0.8392, 5.159, 3.032, 1.502, 4.519, 0],\n",
              " [11.42, 12.86, 0.8683, 5.008, 2.85, 2.7, 4.607, 0],\n",
              " [11.23, 12.63, 0.884, 4.902, 2.879, 2.269, 4.703, 0],\n",
              " [12.36, 13.19, 0.8923, 5.076, 3.042, 3.22, 4.605, 0],\n",
              " [13.22, 13.84, 0.868, 5.395, 3.07, 4.157, 5.088, 0],\n",
              " [12.78, 13.57, 0.8716, 5.262, 3.026, 1.176, 4.782, 0],\n",
              " [12.88, 13.5, 0.8879, 5.139, 3.119, 2.352, 4.607, 0],\n",
              " [14.34, 14.37, 0.8726, 5.63, 3.19, 1.313, 5.15, 0],\n",
              " [14.01, 14.29, 0.8625, 5.609, 3.158, 2.217, 5.132, 0],\n",
              " [14.37, 14.39, 0.8726, 5.569, 3.153, 1.464, 5.3, 0],\n",
              " [12.73, 13.75, 0.8458, 5.412, 2.882, 3.533, 5.067, 0],\n",
              " [17.63, 15.98, 0.8673, 6.191, 3.561, 4.076, 6.06, 1],\n",
              " [16.84, 15.67, 0.8623, 5.998, 3.484, 4.675, 5.877, 1],\n",
              " [17.26, 15.73, 0.8763, 5.978, 3.594, 4.539, 5.791, 1],\n",
              " [19.11, 16.26, 0.9081, 6.154, 3.93, 2.936, 6.079, 1],\n",
              " [16.82, 15.51, 0.8786, 6.017, 3.486, 4.004, 5.841, 1],\n",
              " [16.77, 15.62, 0.8638, 5.927, 3.438, 4.92, 5.795, 1],\n",
              " [17.32, 15.91, 0.8599, 6.064, 3.403, 3.824, 5.922, 1],\n",
              " [20.71, 17.23, 0.8763, 6.579, 3.814, 4.451, 6.451, 1],\n",
              " [18.94, 16.49, 0.875, 6.445, 3.639, 5.064, 6.362, 1],\n",
              " [17.12, 15.55, 0.8892, 5.85, 3.566, 2.858, 5.746, 1],\n",
              " [16.53, 15.34, 0.8823, 5.875, 3.467, 5.532, 5.88, 1],\n",
              " [18.72, 16.19, 0.8977, 6.006, 3.857, 5.324, 5.879, 1],\n",
              " [20.2, 16.89, 0.8894, 6.285, 3.864, 5.173, 6.187, 1],\n",
              " [19.57, 16.74, 0.8779, 6.384, 3.772, 1.472, 6.273, 1],\n",
              " [19.51, 16.71, 0.878, 6.366, 3.801, 2.962, 6.185, 1],\n",
              " [18.27, 16.09, 0.887, 6.173, 3.651, 2.443, 6.197, 1],\n",
              " [18.88, 16.26, 0.8969, 6.084, 3.764, 1.649, 6.109, 1],\n",
              " [18.98, 16.66, 0.859, 6.549, 3.67, 3.691, 6.498, 1],\n",
              " [21.18, 17.21, 0.8989, 6.573, 4.033, 5.78, 6.231, 1],\n",
              " [20.88, 17.05, 0.9031, 6.45, 4.032, 5.016, 6.321, 1],\n",
              " [20.1, 16.99, 0.8746, 6.581, 3.785, 1.955, 6.449, 1],\n",
              " [18.76, 16.2, 0.8984, 6.172, 3.796, 3.12, 6.053, 1],\n",
              " [18.81, 16.29, 0.8906, 6.272, 3.693, 3.237, 6.053, 1],\n",
              " [18.59, 16.05, 0.9066, 6.037, 3.86, 6.001, 5.877, 1],\n",
              " [18.36, 16.52, 0.8452, 6.666, 3.485, 4.933, 6.448, 1],\n",
              " [16.87, 15.65, 0.8648, 6.139, 3.463, 3.696, 5.967, 1],\n",
              " [19.31, 16.59, 0.8815, 6.341, 3.81, 3.477, 6.238, 1],\n",
              " [18.98, 16.57, 0.8687, 6.449, 3.552, 2.144, 6.453, 1],\n",
              " [18.17, 16.26, 0.8637, 6.271, 3.512, 2.853, 6.273, 1],\n",
              " [18.72, 16.34, 0.881, 6.219, 3.684, 2.188, 6.097, 1],\n",
              " [16.41, 15.25, 0.8866, 5.718, 3.525, 4.217, 5.618, 1],\n",
              " [17.99, 15.86, 0.8992, 5.89, 3.694, 2.068, 5.837, 1],\n",
              " [19.46, 16.5, 0.8985, 6.113, 3.892, 4.308, 6.009, 1],\n",
              " [19.18, 16.63, 0.8717, 6.369, 3.681, 3.357, 6.229, 1],\n",
              " [18.95, 16.42, 0.8829, 6.248, 3.755, 3.368, 6.148, 1],\n",
              " [18.83, 16.29, 0.8917, 6.037, 3.786, 2.553, 5.879, 1],\n",
              " [18.85, 16.17, 0.9056, 6.152, 3.806, 2.843, 6.2, 1],\n",
              " [17.63, 15.86, 0.88, 6.033, 3.573, 3.747, 5.929, 1],\n",
              " [19.94, 16.92, 0.8752, 6.675, 3.763, 3.252, 6.55, 1],\n",
              " [18.55, 16.22, 0.8865, 6.153, 3.674, 1.738, 5.894, 1],\n",
              " [18.45, 16.12, 0.8921, 6.107, 3.769, 2.235, 5.794, 1],\n",
              " [19.38, 16.72, 0.8716, 6.303, 3.791, 3.678, 5.965, 1],\n",
              " [19.13, 16.31, 0.9035, 6.183, 3.902, 2.109, 5.924, 1],\n",
              " [19.14, 16.61, 0.8722, 6.259, 3.737, 6.682, 6.053, 1],\n",
              " [20.97, 17.25, 0.8859, 6.563, 3.991, 4.677, 6.316, 1],\n",
              " [19.06, 16.45, 0.8854, 6.416, 3.719, 2.248, 6.163, 1],\n",
              " [18.96, 16.2, 0.9077, 6.051, 3.897, 4.334, 5.75, 1],\n",
              " [19.15, 16.45, 0.889, 6.245, 3.815, 3.084, 6.185, 1],\n",
              " [18.89, 16.23, 0.9008, 6.227, 3.769, 3.639, 5.966, 1],\n",
              " [20.03, 16.9, 0.8811, 6.493, 3.857, 3.063, 6.32, 1],\n",
              " [20.24, 16.91, 0.8897, 6.315, 3.962, 5.901, 6.188, 1],\n",
              " [18.14, 16.12, 0.8772, 6.059, 3.563, 3.619, 6.011, 1],\n",
              " [16.17, 15.38, 0.8588, 5.762, 3.387, 4.286, 5.703, 1],\n",
              " [18.43, 15.97, 0.9077, 5.98, 3.771, 2.984, 5.905, 1],\n",
              " [15.99, 14.89, 0.9064, 5.363, 3.582, 3.336, 5.144, 1],\n",
              " [18.75, 16.18, 0.8999, 6.111, 3.869, 4.188, 5.992, 1],\n",
              " [18.65, 16.41, 0.8698, 6.285, 3.594, 4.391, 6.102, 1],\n",
              " [17.98, 15.85, 0.8993, 5.979, 3.687, 2.257, 5.919, 1],\n",
              " [20.16, 17.03, 0.8735, 6.513, 3.773, 1.91, 6.185, 1],\n",
              " [17.55, 15.66, 0.8991, 5.791, 3.69, 5.366, 5.661, 1],\n",
              " [18.3, 15.89, 0.9108, 5.979, 3.755, 2.837, 5.962, 1],\n",
              " [18.94, 16.32, 0.8942, 6.144, 3.825, 2.908, 5.949, 1],\n",
              " [15.38, 14.9, 0.8706, 5.884, 3.268, 4.462, 5.795, 1],\n",
              " [16.16, 15.33, 0.8644, 5.845, 3.395, 4.266, 5.795, 1],\n",
              " [15.56, 14.89, 0.8823, 5.776, 3.408, 4.972, 5.847, 1],\n",
              " [15.38, 14.66, 0.899, 5.477, 3.465, 3.6, 5.439, 1],\n",
              " [17.36, 15.76, 0.8785, 6.145, 3.574, 3.526, 5.971, 1],\n",
              " [15.57, 15.15, 0.8527, 5.92, 3.231, 2.64, 5.879, 1],\n",
              " [15.6, 15.11, 0.858, 5.832, 3.286, 2.725, 5.752, 1],\n",
              " [16.23, 15.18, 0.885, 5.872, 3.472, 3.769, 5.922, 1],\n",
              " [13.07, 13.92, 0.848, 5.472, 2.994, 5.304, 5.395, 2],\n",
              " [13.32, 13.94, 0.8613, 5.541, 3.073, 7.035, 5.44, 2],\n",
              " [13.34, 13.95, 0.862, 5.389, 3.074, 5.995, 5.307, 2],\n",
              " [12.22, 13.32, 0.8652, 5.224, 2.967, 5.469, 5.221, 2],\n",
              " [11.82, 13.4, 0.8274, 5.314, 2.777, 4.471, 5.178, 2],\n",
              " [11.21, 13.13, 0.8167, 5.279, 2.687, 6.169, 5.275, 2],\n",
              " [11.43, 13.13, 0.8335, 5.176, 2.719, 2.221, 5.132, 2],\n",
              " [12.49, 13.46, 0.8658, 5.267, 2.967, 4.421, 5.002, 2],\n",
              " [12.7, 13.71, 0.8491, 5.386, 2.911, 3.26, 5.316, 2],\n",
              " [10.79, 12.93, 0.8107, 5.317, 2.648, 5.462, 5.194, 2],\n",
              " [11.83, 13.23, 0.8496, 5.263, 2.84, 5.195, 5.307, 2],\n",
              " [12.01, 13.52, 0.8249, 5.405, 2.776, 6.992, 5.27, 2],\n",
              " [12.26, 13.6, 0.8333, 5.408, 2.833, 4.756, 5.36, 2],\n",
              " [11.18, 13.04, 0.8266, 5.22, 2.693, 3.332, 5.001, 2],\n",
              " [11.36, 13.05, 0.8382, 5.175, 2.755, 4.048, 5.263, 2],\n",
              " [11.19, 13.05, 0.8253, 5.25, 2.675, 5.813, 5.219, 2],\n",
              " [11.34, 12.87, 0.8596, 5.053, 2.849, 3.347, 5.003, 2],\n",
              " [12.13, 13.73, 0.8081, 5.394, 2.745, 4.825, 5.22, 2],\n",
              " [11.75, 13.52, 0.8082, 5.444, 2.678, 4.378, 5.31, 2],\n",
              " [11.49, 13.22, 0.8263, 5.304, 2.695, 5.388, 5.31, 2],\n",
              " [12.54, 13.67, 0.8425, 5.451, 2.879, 3.082, 5.491, 2],\n",
              " [12.02, 13.33, 0.8503, 5.35, 2.81, 4.271, 5.308, 2],\n",
              " [12.05, 13.41, 0.8416, 5.267, 2.847, 4.988, 5.046, 2],\n",
              " [12.55, 13.57, 0.8558, 5.333, 2.968, 4.419, 5.176, 2],\n",
              " [11.14, 12.79, 0.8558, 5.011, 2.794, 6.388, 5.049, 2],\n",
              " [12.1, 13.15, 0.8793, 5.105, 2.941, 2.201, 5.056, 2],\n",
              " [12.44, 13.59, 0.8462, 5.319, 2.897, 4.924, 5.27, 2],\n",
              " [12.15, 13.45, 0.8443, 5.417, 2.837, 3.638, 5.338, 2],\n",
              " [11.35, 13.12, 0.8291, 5.176, 2.668, 4.337, 5.132, 2],\n",
              " [11.24, 13.0, 0.8359, 5.09, 2.715, 3.521, 5.088, 2],\n",
              " [11.02, 13.0, 0.8189, 5.325, 2.701, 6.735, 5.163, 2],\n",
              " [11.55, 13.1, 0.8455, 5.167, 2.845, 6.715, 4.956, 2],\n",
              " [11.27, 12.97, 0.8419, 5.088, 2.763, 4.309, 5.0, 2],\n",
              " [11.4, 13.08, 0.8375, 5.136, 2.763, 5.588, 5.089, 2],\n",
              " [10.83, 12.96, 0.8099, 5.278, 2.641, 5.182, 5.185, 2],\n",
              " [10.8, 12.57, 0.859, 4.981, 2.821, 4.773, 5.063, 2],\n",
              " [11.26, 13.01, 0.8355, 5.186, 2.71, 5.335, 5.092, 2],\n",
              " [10.74, 12.73, 0.8329, 5.145, 2.642, 4.702, 4.963, 2],\n",
              " [11.48, 13.05, 0.8473, 5.18, 2.758, 5.876, 5.002, 2],\n",
              " [12.21, 13.47, 0.8453, 5.357, 2.893, 1.661, 5.178, 2],\n",
              " [11.41, 12.95, 0.856, 5.09, 2.775, 4.957, 4.825, 2],\n",
              " [12.46, 13.41, 0.8706, 5.236, 3.017, 4.987, 5.147, 2],\n",
              " [12.19, 13.36, 0.8579, 5.24, 2.909, 4.857, 5.158, 2],\n",
              " [11.65, 13.07, 0.8575, 5.108, 2.85, 5.209, 5.135, 2],\n",
              " [12.89, 13.77, 0.8541, 5.495, 3.026, 6.185, 5.316, 2],\n",
              " [11.56, 13.31, 0.8198, 5.363, 2.683, 4.062, 5.182, 2],\n",
              " [11.81, 13.45, 0.8198, 5.413, 2.716, 4.898, 5.352, 2],\n",
              " [10.91, 12.8, 0.8372, 5.088, 2.675, 4.179, 4.956, 2],\n",
              " [11.23, 12.82, 0.8594, 5.089, 2.821, 7.524, 4.957, 2],\n",
              " [10.59, 12.41, 0.8648, 4.899, 2.787, 4.975, 4.794, 2],\n",
              " [10.93, 12.8, 0.839, 5.046, 2.717, 5.398, 5.045, 2],\n",
              " [11.27, 12.86, 0.8563, 5.091, 2.804, 3.985, 5.001, 2],\n",
              " [11.87, 13.02, 0.8795, 5.132, 2.953, 3.597, 5.132, 2],\n",
              " [10.82, 12.83, 0.8256, 5.18, 2.63, 4.853, 5.089, 2],\n",
              " [12.11, 13.27, 0.8639, 5.236, 2.975, 4.132, 5.012, 2],\n",
              " [12.8, 13.47, 0.886, 5.16, 3.126, 4.873, 4.914, 2],\n",
              " [12.79, 13.53, 0.8786, 5.224, 3.054, 5.483, 4.958, 2],\n",
              " [13.37, 13.78, 0.8849, 5.32, 3.128, 4.67, 5.091, 2],\n",
              " [12.62, 13.67, 0.8481, 5.41, 2.911, 3.306, 5.231, 2],\n",
              " [12.76, 13.38, 0.8964, 5.073, 3.155, 2.828, 4.83, 2],\n",
              " [12.38, 13.44, 0.8609, 5.219, 2.989, 5.472, 5.045, 2],\n",
              " [12.67, 13.32, 0.8977, 4.984, 3.135, 2.3, 4.745, 2],\n",
              " [11.18, 12.72, 0.868, 5.009, 2.81, 4.051, 4.828, 2],\n",
              " [12.7, 13.41, 0.8874, 5.183, 3.091, 8.456, 5.0, 2],\n",
              " [12.37, 13.47, 0.8567, 5.204, 2.96, 3.919, 5.001, 2],\n",
              " [12.19, 13.2, 0.8783, 5.137, 2.981, 3.631, 4.87, 2],\n",
              " [11.23, 12.88, 0.8511, 5.14, 2.795, 4.325, 5.003, 2],\n",
              " [13.2, 13.66, 0.8883, 5.236, 3.232, 8.315, 5.056, 2],\n",
              " [11.84, 13.21, 0.8521, 5.175, 2.836, 3.598, 5.044, 2],\n",
              " [12.3, 13.34, 0.8684, 5.243, 2.974, 5.637, 5.063, 2]]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dff \n",
        "n_inputs = len(dataset[0]) - 1\n",
        "print(n_inputs)\n",
        "n_outputs = len(set([row[-1] for row in dataset]))\n",
        "print(n_outputs)\n",
        "print(dff[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRObUZifWei-",
        "outputId": "6bd4f72d-9cff-4021-f563-cfff2e614aa3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n",
            "3\n",
            "[15.26, 14.84, 0.871, 5.763, 3.312, 2.221, 5.22, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(n_outputs)\n",
        "network = initialize_network(n_inputs, 7, n_outputs)\n",
        "write(network)\n",
        "train_network(network, dff, 0.005, 80, n_outputs)\n",
        "write(network)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p09PhXACWgSZ",
        "outputId": "bbb2fa39-85aa-41cf-fc6e-95c0e9c3ff3f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "layer : 1 \n",
            "\n",
            "{'weights': [0.5, 0.5, 0.7, 0.2, 0.0, 0.8, 0.2, 0.4]} \n",
            "\n",
            "{'weights': [0.7, 0.4, 0.8, 0.8, 0.2, 1.0, 0.3, 0.1]} \n",
            "\n",
            "{'weights': [0.2, 0.1, 0.4, 0.7, 0.2, 0.5, 0.1, 0.9]} \n",
            "\n",
            "{'weights': [0.5, 0.2, 0.9, 0.8, 1.0, 0.7, 0.8, 0.7]} \n",
            "\n",
            "{'weights': [0.4, 0.1, 1.0, 0.9, 0.5, 0.9, 0.8, 0.5]} \n",
            "\n",
            "{'weights': [0.8, 0.0, 0.5, 0.1, 0.1, 0.3, 0.9, 0.4]} \n",
            "\n",
            "{'weights': [0.3, 0.2, 0.4, 0.7, 0.9, 0.1, 0.1, 0.0]} \n",
            "\n",
            "layer : 2 \n",
            "\n",
            "{'weights': [0.3, 0.3, 0.4, 0.5, 0.6, 0.4, 1.0, 0.9]} \n",
            "\n",
            "{'weights': [0.8, 0.8, 0.3, 0.6, 0.1, 0.1, 0.2, 0.9]} \n",
            "\n",
            "{'weights': [0.2, 0.7, 0.9, 0.7, 0.5, 0.3, 0.0, 0.9]} \n",
            "\n",
            ">epoch=0, lrate=0.005, error=405.88040\n",
            ">epoch=1, lrate=0.005, error=404.41455\n",
            ">epoch=2, lrate=0.005, error=402.60083\n",
            ">epoch=3, lrate=0.005, error=400.29596\n",
            ">epoch=4, lrate=0.005, error=397.26586\n",
            ">epoch=5, lrate=0.005, error=393.10416\n",
            ">epoch=6, lrate=0.005, error=387.05872\n",
            ">epoch=7, lrate=0.005, error=377.67866\n",
            ">epoch=8, lrate=0.005, error=362.42993\n",
            ">epoch=9, lrate=0.005, error=339.60256\n",
            ">epoch=10, lrate=0.005, error=315.12430\n",
            ">epoch=11, lrate=0.005, error=296.11308\n",
            ">epoch=12, lrate=0.005, error=278.28252\n",
            ">epoch=13, lrate=0.005, error=255.37327\n",
            ">epoch=14, lrate=0.005, error=228.56518\n",
            ">epoch=15, lrate=0.005, error=202.75608\n",
            ">epoch=16, lrate=0.005, error=176.27221\n",
            ">epoch=17, lrate=0.005, error=153.89757\n",
            ">epoch=18, lrate=0.005, error=143.44775\n",
            ">epoch=19, lrate=0.005, error=140.38852\n",
            ">epoch=20, lrate=0.005, error=139.66325\n",
            ">epoch=21, lrate=0.005, error=139.55066\n",
            ">epoch=22, lrate=0.005, error=139.58721\n",
            ">epoch=23, lrate=0.005, error=139.64849\n",
            ">epoch=24, lrate=0.005, error=139.70251\n",
            ">epoch=25, lrate=0.005, error=139.74346\n",
            ">epoch=26, lrate=0.005, error=139.77264\n",
            ">epoch=27, lrate=0.005, error=139.79283\n",
            ">epoch=28, lrate=0.005, error=139.80655\n",
            ">epoch=29, lrate=0.005, error=139.81580\n",
            ">epoch=30, lrate=0.005, error=139.82199\n",
            ">epoch=31, lrate=0.005, error=139.82611\n",
            ">epoch=32, lrate=0.005, error=139.82886\n",
            ">epoch=33, lrate=0.005, error=139.83068\n",
            ">epoch=34, lrate=0.005, error=139.83189\n",
            ">epoch=35, lrate=0.005, error=139.83269\n",
            ">epoch=36, lrate=0.005, error=139.83322\n",
            ">epoch=37, lrate=0.005, error=139.83358\n",
            ">epoch=38, lrate=0.005, error=139.83381\n",
            ">epoch=39, lrate=0.005, error=139.83396\n",
            ">epoch=40, lrate=0.005, error=139.83407\n",
            ">epoch=41, lrate=0.005, error=139.83413\n",
            ">epoch=42, lrate=0.005, error=139.83418\n",
            ">epoch=43, lrate=0.005, error=139.83421\n",
            ">epoch=44, lrate=0.005, error=139.83423\n",
            ">epoch=45, lrate=0.005, error=139.83424\n",
            ">epoch=46, lrate=0.005, error=139.83425\n",
            ">epoch=47, lrate=0.005, error=139.83425\n",
            ">epoch=48, lrate=0.005, error=139.83426\n",
            ">epoch=49, lrate=0.005, error=139.83426\n",
            ">epoch=50, lrate=0.005, error=139.83426\n",
            ">epoch=51, lrate=0.005, error=139.83426\n",
            ">epoch=52, lrate=0.005, error=139.83426\n",
            ">epoch=53, lrate=0.005, error=139.83426\n",
            ">epoch=54, lrate=0.005, error=139.83427\n",
            ">epoch=55, lrate=0.005, error=139.83427\n",
            ">epoch=56, lrate=0.005, error=139.83427\n",
            ">epoch=57, lrate=0.005, error=139.83427\n",
            ">epoch=58, lrate=0.005, error=139.83427\n",
            ">epoch=59, lrate=0.005, error=139.83427\n",
            ">epoch=60, lrate=0.005, error=139.83427\n",
            ">epoch=61, lrate=0.005, error=139.83427\n",
            ">epoch=62, lrate=0.005, error=139.83427\n",
            ">epoch=63, lrate=0.005, error=139.83427\n",
            ">epoch=64, lrate=0.005, error=139.83427\n",
            ">epoch=65, lrate=0.005, error=139.83427\n",
            ">epoch=66, lrate=0.005, error=139.83427\n",
            ">epoch=67, lrate=0.005, error=139.83427\n",
            ">epoch=68, lrate=0.005, error=139.83427\n",
            ">epoch=69, lrate=0.005, error=139.83427\n",
            ">epoch=70, lrate=0.005, error=139.83427\n",
            ">epoch=71, lrate=0.005, error=139.83427\n",
            ">epoch=72, lrate=0.005, error=139.83427\n",
            ">epoch=73, lrate=0.005, error=139.83427\n",
            ">epoch=74, lrate=0.005, error=139.83427\n",
            ">epoch=75, lrate=0.005, error=139.83427\n",
            ">epoch=76, lrate=0.005, error=139.83427\n",
            ">epoch=77, lrate=0.005, error=139.83427\n",
            ">epoch=78, lrate=0.005, error=139.83427\n",
            ">epoch=79, lrate=0.005, error=139.83427\n",
            "layer : 1 \n",
            "\n",
            "{'weights': [0.4999998157042541, 0.49999979746175116, 0.6999999869301962, 0.19999992070508793, -4.431159481812129e-08, 0.7999999535859609, 0.1999999239726988, 0.39999998477231574], 'output': 0.999999998616534, 'delta': -7.191939865757066e-11} \n",
            "\n",
            "{'weights': [0.6999999994453178, 0.39999999941031905, 0.7999999999618406, 0.7999999997724752, 0.19999999986706846, 0.9999999999233201, 0.2999999997899238, 0.09999999995634291], 'output': 0.9999999999974267, 'delta': 5.410171583579521e-14} \n",
            "\n",
            "{'weights': [0.2000045880645308, 0.100023197921068, 0.40000192611679114, 0.7000105616817339, 0.20000334273128284, 0.5000286346641202, 0.10001312617733911, 0.9000026111685808], 'output': 0.999996733286326, 'delta': 2.639410233663097e-07} \n",
            "\n",
            "{'weights': [0.4999999997525539, 0.19999999974656368, 0.8999999999837682, 0.7999999999031535, 0.9999999999416537, 0.6999999999818611, 0.7999999999133693, 0.6999999999816962], 'output': 0.9999999999912701, 'delta': 2.0653160084387858e-13} \n",
            "\n",
            "{'weights': [0.4000000146879238, 0.10000001571910236, 1.0000000010098498, 0.9000000060986895, 0.5000000035045614, 0.9000000027321537, 0.8000000057096808, 0.5000000011639731], 'output': 0.9999999998923139, 'delta': 2.6948660383251025e-12} \n",
            "\n",
            "{'weights': [0.8000002657889766, 2.9077602918936804e-07, 0.5000000188070861, 0.10000011361988818, 0.10000006405567873, 0.3000000711478801, 0.9000001077001711, 0.4000000218424874], 'output': 0.9999999803209051, 'delta': 1.5379357498603377e-10} \n",
            "\n",
            "{'weights': [0.2999870654926858, 0.19998396301023724, 0.399998954214139, 0.6999934996940635, 0.8999967577177651, 0.09999157796140325, 0.09999326558019966, -1.2785947100960067e-06], 'output': 0.9999992636500463, 'delta': -5.838206739182805e-08} \n",
            "\n",
            "layer : 2 \n",
            "\n",
            "{'weights': [-0.3617452363812142, -0.36174522510798773, -0.2617586907173746, -0.16174522510710834, -0.061745227250402, -0.26174520688817193, 0.33825562694655936, 0.23825477493642627], 'output': 0.29078489551846887, 'delta': -0.059968289865785486} \n",
            "\n",
            "{'weights': [0.23639981132833898, 0.23639978401608666, -0.2635710168023041, 0.036399784069595674, -0.4636002128630881, -0.4636000790240755, -0.36359693930668674, 0.33639978391963055], 'output': 0.3305181407824447, 'delta': -0.07313569886838234} \n",
            "\n",
            "{'weights': [-0.38513935977782754, 0.11486064672613511, 0.3148569572551314, 0.11486064668887053, -0.08513935311747978, -0.2851394657778381, -0.5851424234115398, 0.31486064674519104], 'output': 0.3806092578048419, 'delta': 0.14601879742093063} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for row in dataset:\n",
        "\tprediction = predict(network, row)\n",
        "\tprint('Expected=%d, Got=%d' % (row[-1], prediction))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6iV13RrXGtq",
        "outputId": "735993b2-90dc-4fbc-aca6-044e874b29ec"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=0, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=1, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n",
            "Expected=2, Got=2\n"
          ]
        }
      ]
    }
  ]
}